%% Module 2 beamer/knitr slides
%% Biostatistics in Practice workshop, January 2014
%% Nicholas Reich: nick [at] schoolph.umass.edu


\documentclass[table]{beamer}


\input{../../slide-includes/standard-knitr-beamer-preamble}

%        The following variables are assumed by the standard preamble:
%	Global variable containing module name:
\title{Using splines in regression}
%	Global variable containing module shortname:
%		(Currently unused, may be used in future.)
\newcommand{\ModuleShortname}{MLR}
%	Global variable containing author name:
\author{Nicholas G Reich, adapted from content by Andrea Foulkes}
%	Global variable containing text of license terms:
\newcommand{\LicenseText}{Made available under the Creative Commons Attribution-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-sa/3.0/deed.en\_US }
%	Instructor: optional, can leave blank.
%		Recommended format: {Instructor: Jane Doe}
\newcommand{\Instructor}{}
%	Course: optional, can leave blank.
%		Recommended format: {Course: Biostatistics 101}
\newcommand{\Course}{}

\input{../../slide-includes/shortcuts}
\usepackage{bbm}
\hypersetup{colorlinks=TRUE, urlcolor=blue}

%%%%%%%% IMPORTANT -- MUST HAVE [fragile] for some/all frames chunks to have output work correctly. 

\begin{document}

<<setup, include=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figs/beamer-',fig.align='center',fig.show='hold',size='footnotesize', message=FALSE, fig.height=3.5)
@


\begin{frame}[plain]
        \titlepage
\end{frame}

<<ggplot2, echo=FALSE, message=FALSE, warning=FALSE>>=
require(ggplot2)
theme_set(theme_bw())
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's Lecture}

\bi
    \myitem Assessing model accuracy
    \myitem Overfitting
    \myitem Fitting smooth curves to data
\ei

\vspace{2em}

More info: 
\bi
    \myitem For more gory details: {\em Intro to Statistical Learning} Chapter 7
\ei


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Assessing model accuracy: Regression setting}
\begin{itemize}
\item The {\color{orange} mean squared error (MSE)} is the most commonly used measure of the performance of a statistical learning method in the regression setting:
%
\begin{eqnarray*}
MSE = \frac{1}{n} \sum_{i=1}^n \left(y_i -\hat y_i\right)^2
\end{eqnarray*}

\bigskip
\item The MSE will be small if the observed and predicted responses are close to one another, and it will be large if these differ substantially, in at least some instances.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Low MSE is not always a good thing!}
\begin{itemize}
  \item Which color line/model do you think has the lowest MSE?
  \item Which color line/model would you trust the most to predict a new observation?
\end{itemize}

\centerline{
\includegraphics[scale=.6,angle=0]{figure-static/2_9-half.pdf}
}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{How to find the ``sweet spot''?}
\begin{itemize}
  \item Ideally, your model should predict observations well in your dataset (``in-sample'') and in a new dataset (``out-of-sample''). 
  \item While this is not often possible in practice, ideally the new dataset should be completely separately collected from the initial dataset.
  \item Example 1: a study done by investigators at Institution A shows that in a sample of 100 cancer patients, a specific set of commonly collected biomarkers accurately classified cancer patients based on severity of outcome at 1 year follow-up. What would be a good replication study?
  % \item Example 2: using photos from a common social media platform, authors claimed that their face recognition algorithm could accurately predict the sexual orientation of individuals based solely on the photo. What would be a good replication study?
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Often, an `internal` test sample is used}
\begin{itemize}
  \item In general, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen data $\rightarrow$  called the {\color{violet} test sample}. \\
  \item Often, obtaining an external validation or testing dataset is infeasible or expensive.
  \item In these cases, investigators may set aside a portion of the observations from the original dataset as a {\color{violet} test sample}.
  \item Test samples can be particularly useful in diagnosing whether your model was ``overfit`` to the particular features of the training dataset.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Hands-on example: FEV dataset}

The FEV dataset describes a sample of 654 youths, aged 3 to 19, in the area of East Boston during middle to late 1970's. Data includes the following variables (among others)

\begin{itemize}
  \item fev - forced expiratory volume, a measure of lung capacity and strength (in liters)
  \item height (in inches)
\end{itemize}

\scriptsize
<<>>=
library("Hmisc")
getHdata(FEV)
head(FEV)
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Creating an FEV test sample}

\scriptsize
<<>>=
set.seed(756)  ## so we all get the same result
idx_test <- sample(1:nrow(FEV), size=200) ## 200 test observations
fev_train <- FEV[-idx_test,]    ## removing all test observations 
fev_test <- FEV[idx_test,]      ## leaving all test observations
ggplot(fev_train, aes(height, fev)) + geom_point() + 
  geom_smooth(method="lm", se=FALSE)
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Using a smooth model for $fev \sim height$}

Let's fit a model to the data that can capture some of what appears to be a non-linear relationship between height and fev.

The {\tt ns()} function in the splines package uses a "natural spline" to model a flexible, non-linear relationship bewteen a covariate and an outcome by splicing together polynomial functions. The larger the ``degrees of freedom'' ({\tt df}) parameter is, the more functions it pieces together and therefore the more wiggly the fitted model becomes. 

\scriptsize
<<>>=
library(splines)
spline_mdl <- lm(fev ~ ns(height, df = 4), data=fev_train)
@

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Visualizing the smooth model}


\scriptsize
<<>>=
spline_x_vals <- seq(45, 75, by=1)
spline_y_vals <- predict(spline_mdl,
  newdata = data.frame(height=spline_x_vals))
ggplot() + 
  geom_point(aes(x=height, y=fev), data=fev_train) + 
  geom_line(aes(x = spline_x_vals, y=spline_y_vals), color="red")
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Find the Goldilocks zone!}

That model looks good, but how to we know what degree of smoothness is just the right amount? We can compare MSE between our training and our test set for different levels of smoothness.

\scriptsize
<<>>=
fev_train$spline_preds <- predict(spline_mdl)
fev_test$spline_preds <- predict(spline_mdl, newdata = fev_test)
( mse_train <- mean((fev_train$fev - fev_train$spline_preds)^2) )
( mse_test <- mean((fev_test$fev - fev_test$spline_preds)^2) )
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{With your group}

\begin{itemize}
\item In your breakout rooms, pick a set of {\tt df} to test as a group. Work together to make sure that at least one {\tt df} is run on everyone's computer. 
\item Compile a dataset on the note-catcher with the {\tt df} value and the associated MSEs for both the training and testing samples. 
\item Discuss your results as a table and decide on an optimal {\tt df} to choose.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Using a test sample avoids ``overfitting''}
\begin{itemize}
  \item A good rule of thumb of a good model is that it will generalize to another setting.
  \item Therefore, a good model is one that {\bf minimizes the test sample MSE.}
  \item The mean squared error will always get lower with the training sample as we add more features to our model.
  \item {\color{orange} Overfitting} refers to the situation in which a less complex or ``wiggly'' model would result in a smaller test sample MSE.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{MSE: learning versus test sample}
\centerline{
\includegraphics[scale=.6,angle=0]{figure-static/2_9-all.pdf}
}

Figure 2.9 from ISL. Black line is the true relationship from which data are simulated
\end{frame}

\begin{frame}{MSE: learning versus test sample}
\centerline{
\includegraphics[scale=.6,angle=0]{figure-static/2_10.pdf}
}
Figure 2.10 from ISL.

\end{frame}


\begin{frame}{Summary}

Validating potential models on external data is critical to understanding how well your model will generalize to another dataset.

\end{frame}



\end{document}
